tink:
  machines:
    - Name: node2
      MAC: 00:50:56:81:85:42
      IPAddress: 172.16.20.234

    - Name: node3
      MAC: 00:50:56:81:9c:e2
      IPAddress: 172.16.20.235

    - Name: node4
      MAC: 00:50:56:81:52:48
      IPAddress: 172.16.20.236

kubernetes:
  clusterName: k0s.resolvemy.host

  hostClusterDomain: cluster.local

  controlPlaneEndpoint:
  networking:
    dnsDomain: k0s.resolvemy.host
    serviceSubnet: 10.0.8.0/22
    podSubnet: 10.4.0.0/16

  virtual:
    enabled: false

  kine:
    enabled: true

  persistence:
    enabled: false
    accessModes:
      - ReadWriteOnce
    size: 1Gi
    # storageClassName: default
    annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection

    backup:
      # existingClaim: your-claim
      # subPath: backups
      accessModes:
        - ReadWriteOnce
      size: 1Gi
      # storageClassName: default
      annotations: {}
      finalizers:
        - kubernetes.io/pvc-protection

  etcd:
    enabled: true

    image:
      repository: k8s.gcr.io/etcd
      tag: 3.5.3-0
      pullPolicy: IfNotPresent
      pullSecrets: []

    replicaCount: 2

    resources:
      requests:
        cpu: 1024m
        memory: 2048Mi

      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    certSANs:
      dnsNames: []
      ipAddresses: []

    extraArgs: {}
    labels: {}

    annotations: {}
    podLabels: {}

    podAnnotations: {}

    nodeSelector: {}

    tolerations: []

    podAntiAffinity: soft

    podAntiAffinityTopologyKey: kubernetes.io/hostname

    affinity: {}

    extraEnv: []

    sidecars: []

    extraVolumes:
      - name: etcd-snapshot
        hostPath:
          # directory location on host
          path: /mnt/Site1.NAS1.Pool1/Backups/etcd-snapshot.db
          # this field is optional
          type: File

    extraVolumeMounts:
      - mountPath: /backup/snapshot.db
        name: etcd-snapshot

    ports:
      client: 2379
      peer: 2380
      metrics: 2381

    service:
      enabled: true
      type: ClusterIP
      ports:
        client: 2379
        peer: 2380
        metrics: 2381
      labels: {}
      annotations: {}
      loadBalancerIP:

    backup:
      enabled: false
      schedule: "0 */12 * * *"
      successfulJobsHistoryLimit: 3
      failedJobsHistoryLimit: 3
      extraArgs: #{}
        debug: true
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        # limits:
        #   cpu: 100m
        #   memory: 128Mi

      labels: {}
      annotations: {}
      podLabels: {}
      podAnnotations: {}
      nodeSelector: {}
      tolerations: []
      podAffinity: soft
      podAffinityTopologyKey: kubernetes.io/hostname
      affinity: {}
      extraEnv: []
      sidecars: []
      extraVolumes: []
      extraVolumeMounts: []

  apiServer:
    enabled: true

    image:
      repository: k8s.gcr.io/kube-apiserver
      tag: v1.23.6
      pullPolicy: IfNotPresent
      pullSecrets: []

    replicaCount: 3
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    certSANs:
      dnsNames: ['k0s-kubeapi.service.dc1.kjdev', 'k0s-kubeapi.service.kjdev', 'api.k0s.dc1.resolvemy.host', 'kubernetes.default', 'kubernetes.default.svc', 'kubernetes.default.svc.cluster.local', 'kubernetes.default.svc.cluster']
      ipAddresses: ['172.18.213.183', '10.1.1.40', '10.0.8.1', '10.1.1.41']

    extraArgs:
      feature-gates: MixedProtocolLBService=true

    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []


    port: 6443

    service:
      enabled: true
      type: LoadBalancer # NodePort / LoadBalancer
      port: 6443
      # Specify nodePort for apiserver service (30000-32767)
      nodePort:
      labels: {}

      externalIPs:
        - 10.0.8.1

      annotations:
        consul.hashicorp.com/service-name: k0s-kubeapi
        metallb.universe.tf/allow-shared-ip: k0s-api

      loadBalancerIP: 10.1.1.40

  controllerManager:
    enabled: true

    image:
      repository: k8s.gcr.io/kube-controller-manager
      tag: v1.23.6
      pullPolicy: IfNotPresent
      pullSecrets: []

    replicaCount: 3

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    extraArgs: {}
    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []

    port: 10257

    service:
      enabled: true
      type: ClusterIP
      port: 10257
      labels: {}
      annotations: {}      
      loadBalancerIP: ''

  scheduler:
    enabled: true

    image:
      repository: k8s.gcr.io/kube-scheduler
      tag: v1.23.6
      pullPolicy: IfNotPresent
      pullSecrets: []

    replicaCount: 1

    resources:
      requests:
        cpu: 100m
        memory: 364Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    extraArgs: {}
    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []

    port: 10259

    service:
      enabled: true
      type: ClusterIP
      port: 10259
      labels: {}
      annotations: {}

      loadBalancerIP: ''

  admin:
    enabled: false

    image:
      repository: ghcr.io/kvaps/kubernetes-tools
      tag: v0.13.4
      pullPolicy: IfNotPresent
      pullSecrets: []
    replicaCount: 1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    job:
      enabled: true
      schedule: "0 0 1 */6 *"
      successfulJobsHistoryLimit: 3
      failedJobsHistoryLimit: 3

    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []

  kubeProxy:
    enabled: false

  coredns:
    enabled: false
    image:
      repository: coredns/coredns
      tag: 1.8.6
      pullPolicy: IfNotPresent
      pullSecrets: []
    replicaCount: 0
    resources:
      limits:
        memory: 170Mi
      requests:
        cpu: 100m
        memory: 70Mi

  konnectivityServer:
    enabled: true
    # This controls the protocol between the API Server and the Konnectivity server.
    # Supported values are "GRPC" and "HTTPConnect".
    # "GRPC" will deploy konnectivity-server as a sidecar for apiserver
    # "HTTPConnect" will deploy konnectivity-server as separate deployment
    mode: HTTPConnect
    image:
      repository: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-server
      tag: v0.0.32
      pullPolicy: IfNotPresent
      pullSecrets: []

    replicaCount: 1
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi

    certSANs:
      dnsNames: ['k0s-kubeapi.service.dc1.kjdev', 'k0s-kubeapi.service.kjdev', 'api.k0s.dc1.resolvemy.host', 'kubernetes.default', 'kubernetes.default.svc', 'kubernetes.default.svc.cluster.local', 'kubernetes.default.svc.cluster']
      ipAddresses: ['172.18.213.183', '10.1.1.40', '10.0.8.1', '10.1.1.41']

    extraArgs: {}
    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []

    ports:
      server: 8131
      agent: 8132
      admin: 8133
      health: 8134

    service:
      enabled: true
      type: LoadBalancer

      ports:
        server: 8131
        agent: 8132
        admin: 8133

      externalIPs:
        - 10.1.1.41

      labels: {}

      annotations:
        metallb.universe.tf/allow-shared-ip: k0s-api
        consul.hashicorp.com/service-name: k0s-konnectivity

      loadBalancerIP: 10.1.1.40

  konnectivityAgent:
    enabled: false
    image:
      repository: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent
      tag: v0.0.30
      pullPolicy: IfNotPresent
      pullSecrets: []
    replicaCount: 1
    hostNetwork: false

    extraArgs:
      proxy-server-host: '10.1.1.40'
      proxy-server-port: '8132'
    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    podAntiAffinity: soft
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    affinity: {}
    extraEnv: []
    sidecars: []
    extraVolumes: []
    extraVolumeMounts: []

    ports:
      admin: 8133
      health: 8134

  # these manifests will be applied inside the cluster
  extraManifests: {}
    #namespace.yaml:
    #  apiVersion: v1
    #  kind: Namespace
    #  metadata:
    #    name: example

kubefarm:
  enabled: false
  # ------------------------------------------------------------------------------
  # Kubernetes control-plane
  # ------------------------------------------------------------------------------
  kubernetes:
    enabled: false
    # See all available options for kubernetes-in-kubernetes chart on:
    # https://github.com/kvaps/kubernetes-in-kubernetes/blob/master/deploy/helm/kubernetes/values.yaml

    persistence:
      enabled: true
      storageClassName: local-path

    apiServer:
      service:
        type: LoadBalancer
        annotations: {}
        loadBalancerIP:

      # Specify external endpoints here to have an oportunity to the Kubernetes cluster externaly
      #certSANs:
      #  ipAddresses:
      #  - 10.9.8.10
      #  dnsNames:
      #  - mycluster.example.org
      #extraArgs:
      #  # advertise-address is required for kube-proxy
      #  advertise-address: 10.9.8.10

  # ------------------------------------------------------------------------------
  # Kubeadm token generator
  # (tokens are needed to join nodes to the cluster)
  # ------------------------------------------------------------------------------
  tokenGenerator:
    enabled: false
    tokenTtl: 24h0m0s
    schedule: "0 */12 * * *"

  # ------------------------------------------------------------------------------
  # Network boot server configuration
  # ------------------------------------------------------------------------------
  ltsp:
    enabled: true
    image:
      repository: ghcr.io/kvaps/kubefarm-ltsp
      tag: v0.13.4
      pullPolicy: IfNotPresent
      pullSecrets: []
    replicaCount: 1

    publishDHCP: true
    service:
      enabled: true
      type: ClusterIP
      loadBalancerIP:
      labels: {}
      annotations: {}

    labels: {}
    annotations: {}
    podLabels: {}
    podAnnotations: {}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    sidecars: []
    extraVolumes: []

    config:

      # Disable ubuntu apt auto-update task
      disableAutoupdate: true

      # from /usr/share/zoneinfo/<timezone> (eg. Europe/Moscow)
      #timezone: UTC

      # SSH-keys authorized to access the nodes
      sshAuthorizedKeys: []
        #- ssh-rsa AAAAB3N...

      # Hashed password for root, use `openssl passwd -1` to generate one
      #rootPasswd: $1$jaKnTiEb$IhpsNUfssXQ8eQg8orald0 # hackme

      # Sysctl setting for the nodes
      sysctls:
        net.ipv4.ip_forward: 1
        net.ipv6.conf.all.forwarding: 1

      # Modules to load during startup
      modules:
        - br_netfilter
        #- ip_vs
        #- ip_vs_rr
        #- ip_vs_wrr
        #- ip_vs_sh

      # Docker configuration file
      dockerConfig:
        exec-opts: [ native.cgroupdriver=systemd ]
        iptables: false
        ip-forward: false
        bridge: none
        log-driver: journald
        storage-driver: overlay2

      # Extra options for ltsp.conf
      options:
        MENU_TIMEOUT: 0
        #KERNEL_PARAMETERS: "forcepae console=tty1 console=ttyS0,9600n8"

      # Extra sections for ltsp.conf
      sections: {}
        #init/: |
        #  cp -f /etc/ltsp/journald.conf /etc/systemd/journald.conf
        #initrd-bottom/: |
        #  echo "Hello World!"

      # These files will be copied into /etc/ltsp directory for all clients
      # Note: all *.service files will be copied and enabled via systemd
      extraFiles: {}
        #journald.conf: |
        #   [Manager]
        #   SystemMaxUse=200M
        #   RuntimeMaxUse=200M

      # Optionaly you might want to map additional ConfigMaps or Secrets 
      # they will be projected to /etc/ltsp drirectory
      extraProjectedItems: []
        #- secret:
        #    name: ipa-join-token


  # ------------------------------------------------------------------------------
  # Nodes configuration
  # ------------------------------------------------------------------------------
  nodePools:
    -
      # DHCP range for the node pool, required for issuing leases.
      # See --dhcp-range option syntax on dnsmasq-man page.
      # Note: the range will automatically be appended with the set:{{ .Release.Name }}-ltsp option.
      #
      # examples:
      #   172.16.0.0,static,infinite
      #   172.16.0.1,172.16.0.100,255.255.255.0,172.16.0.255,1h
      #
      # WARNING setting broadcast-address is required! (see: https://www.mail-archive.com/dnsmasq-discuss@lists.thekelleys.org.uk/msg14137.html)
      range: "172.16.100.1,172.16.100.100,255.255.255.0,172.16.100.255,1h"

      # DHCP configuration for each node
      nodes: 
        - name: node0
          mac: b8:ae:ed:79:5e:1d
          ip: 172.16.100.10

      # Extra tags applied to this node pool, tags may contain additional
      # DHCP- and LTSP- options as well node labels and taints
      tags:
        - debug
        #- foo

      # Extra Options for Dnsmasq. This section can be used to setup Circuit ID matching.
      # Note: Symbol '?' will automatically be replaced by the '{{ .Release.Name }}-ltsp' tag.
      extraOpts: []
        #- dhcp-circuitid: "set:port_0,ge-0/0/0.0:staging"
        #- dhcp-circuitid: "set:port_1,ge-0/0/1.0:staging"
        #- dhcp-range: "set:?,tag:port_0,192.168.11.10,192.168.11.10,255.255.255.0"
        #- dhcp-range: "set:?,tag:port_1,192.168.11.11,192.168.11.11,255.255.255.0"
        #- tag-if: "set:bar,tag:?,tag:port_0"
        #- tag-if: "set:bar,tag:?,tag:port_1"

  # ------------------------------------------------------------------------------
  # Extra options can be specified for each tag
  # ("all" options are aplicable for any node)
  # ------------------------------------------------------------------------------
  tags:
    dhcpOptions:
      # dnsmasq options
      # see all available options list (https://git.io/JJ0dH)
      all: {}
        #router: 172.16.0.1
        #dns-server: 8.8.8.8
        #broadcast: 172.16.0.255
        #domain-search: infra.example.org

    ltspOptions:
      all:
        FSTAB_KUBELET: "tmpfs /var/lib/kubelet tmpfs x-systemd.wanted-by=kubelet.service 0 0"
        #FSTAB_DOCKER: "tmpfs /var/lib/docker tmpfs x-systemd.wanted-by=docker.service 0 0"
        KUBELET_EXTRA_ARGS_CGROUP: "--cgroup-driver=systemd"
      debug:
        DEBUG_SHELL: "1"

    kubernetesLabels:
      all: {}
      #foo:
      #  label1: value1
      #  label2: value2

    kubernetesTaints:
      all: {}
      foo:
        - effect: NoSchedule
          key: foo
          value: bar


vcluster-k8s:
  # DefaultImageRegistry will be prepended to all deployed vcluster images, such as the vcluster pod, coredns etc.. Deployed
  # images within the vcluster will not be rewritten.
  defaultImageRegistry: ""

  # If the control plane is deployed in high availability mode
  # Make sure to scale up the syncer.replicas, etcd.replicas, api.replicas & controller.replicas
  enableHA: false

  # Plugins that should get loaded. Usually you want to apply those via 'vcluster create ... -f https://.../plugin.yaml'
  plugin: {}
  # Manually configure a plugin called test
  # test:
  #   image: ...
  #   env: ...
  #   rbac:
  #     clusterRole:
  #       extraRules: ...
  #     role:
  #       extraRules: ...

  # Resource syncers that should be enabled/disabled.
  # Enabling syncers will impact RBAC Role and ClusterRole permissions.
  # To disable a syncer set "enabled: false".
  # See docs for details - https://www.vcluster.com/docs/architecture/synced-resources
  sync:
    services:
      enabled: true

    configmaps:
      enabled: true

    secrets:
      enabled: true

    endpoints:
      enabled: true

    pods:
      enabled: true

    events:
      enabled: true

    persistentvolumeclaims:
      enabled: true

    ingresses:
      enabled: true

    fake-nodes:
      enabled: true # will be ignored if nodes.enabled = true

    fake-persistentvolumes:
      enabled: true # will be ignored if persistentvolumes.enabled = true

    nodes:
      enabled: true

      # If nodes sync is enabled, and syncAllNodes = true, the virtual cluster 
      # will sync all nodes instead of only the ones where some pods are running. 
      syncAllNodes: true

      # nodeSelector is used to limit which nodes get synced to the vcluster,
      # and which nodes are used to run vcluster pods. 
      # A valid string representation of a label selector must be used. 
      nodeSelector: ""

      # if true, vcluster will run with a scheduler and node changes are possible
      # from within the virtual cluster. This is useful if you would like to
      # taint, drain and label nodes from within the virtual cluster
      enableScheduler: true

      # DEPRECATED: use enable scheduler instead
      # syncNodeChanges allows vcluster user edits of the nodes to be synced down to the host nodes.
      # Write permissions on node resource will be given to the vcluster.
      syncNodeChanges: false

    persistentvolumes:
      enabled: false

    storageclasses:
      enabled: false

    legacy-storageclasses:
      enabled: false

    priorityclasses:
      enabled: false

    networkpolicies:
      enabled: false

    volumesnapshots:
      enabled: false

    poddisruptionbudgets:
      enabled: false

    serviceaccounts:
      enabled: false


  # Map Services between host and virtual cluster
  mapServices:
    # Services that should get mapped from the
    # virtual cluster to the host cluster.
    # vcluster will make sure to sync the service
    # ip to the host cluster automatically as soon
    # as the service exists.
    # For example:
    # fromVirtual:
    #   from: my-namespace/name
    #   to: host-service
    fromVirtual: []

    # Same as from virtual, but instead sync services
    # from the host cluster into the virtual cluster.
    # If the namespace does not exist, vcluster will
    # also create the namespace for the service.
    fromHost: []

  # Syncer configuration
  syncer:
    # Image to use for the syncer
    # image: loftsh/vcluster
    extraArgs: []

    volumeMounts:
      - mountPath: /pki
        name: certs
        readOnly: true

    env: []

    livenessProbe:
      enabled: true

    readinessProbe:
      enabled: true

    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 10m
        memory: 64Mi

    # Extra volumes 
    volumes: []

    # The amount of replicas to run the deployment with
    replicas: 1

    # NodeSelector used to schedule the syncer
    nodeSelector: {}

    # Affinity to apply to the syncer deployment
    affinity: {}

    # Tolerations to apply to the syncer deployment
    tolerations: []

    # Extra Labels for the syncer deployment
    labels: {}

    # Extra Annotations for the syncer deployment
    annotations: {}

    priorityClassName: ""

    kubeConfigContextName: "my-vcluster"

  # Etcd settings
  etcd:
    image: k8s.gcr.io/etcd:3.5.1-0
    # The amount of replicas to run 
    replicas: 1
    # NodeSelector used
    nodeSelector: {}
    # Affinity to apply
    affinity: {}
    # Tolerations to apply
    tolerations: []
    # Extra Labels 
    labels: {}
    # Extra Annotations
    annotations: {}
    resources:
      requests:
        cpu: 20m
        memory: 150Mi
    # Storage settings for the etcd
    storage:
      # If this is disabled, vcluster will use an emptyDir instead
      # of a PersistentVolumeClaim
      persistence: true
      # Size of the persistent volume claim
      size: 5Gi
      # Optional StorageClass used for the pvc
      # if empty default StorageClass defined in your host cluster will be used
      #className:
    priorityClassName: ""

  # Kubernetes Controller Manager settings
  controller:
    image: k8s.gcr.io/kube-controller-manager:v1.24.1
    # The amount of replicas to run the deployment with
    replicas: 1
    # NodeSelector used 
    nodeSelector: {}
    # Affinity to apply 
    affinity: {}
    # Tolerations to apply 
    tolerations: []
    # Extra Labels 
    labels: {}
    # Extra Annotations
    annotations: {}
    resources:
      requests:
        cpu: 15m
    priorityClassName: ""

  # Kubernetes Scheduler settings. Only enabled if sync.nodes.enableScheduler is true
  scheduler:
    image: k8s.gcr.io/kube-scheduler:v1.24.1
    # The amount of replicas to run the deployment with
    replicas: 1
    # NodeSelector used
    nodeSelector: {}
    # Affinity to apply
    affinity: {}
    # Tolerations to apply
    tolerations: []
    # Extra Labels
    labels: {}
    # Extra Annotations
    annotations: {}
    resources:
      requests:
        cpu: 10m
    priorityClassName: ""

  # Kubernetes API Server settings
  api:
    image: k8s.gcr.io/kube-apiserver:v1.24.1
    extraArgs: []
    # The amount of replicas to run the deployment with
    replicas: 1
    # NodeSelector used to schedule the syncer
    nodeSelector: {}
    # Affinity to apply to the syncer deployment
    affinity: {}
    # Tolerations to apply to the syncer deployment
    tolerations: []
    # Extra Labels for the syncer deployment
    labels: {}
    # Extra Annotations for the syncer deployment
    annotations: {}
    resources:
      requests:
        cpu: 40m
        memory: 300Mi
    priorityClassName: ""

  # Service account that should be used by the vcluster
  serviceAccount:
    create: true
    # Optional name of the service account to use
    # name: default

  # Roles & ClusterRoles for the vcluster
  rbac:
    clusterRole:
      # Deprecated ! 
      # Necessary cluster roles are created based on the enabled syncers (.sync.*.enabled)
      # Support for this value will be removed in a future version of the vcluster
      create: false
    role:
      # Deprecated !
      # Support for this value will be removed in a future version of the vcluster
      # and basic role will always be created
      create: true
      # Deprecated ! 
      # Necessary extended roles are created based on the enabled syncers (.sync.*.enabled)
      # Support for this value will be removed in a future version of the vcluster
      extended: false
  # Service configurations
  service:
    type: ClusterIP
    # Configuration for LoadBalancer service type
    externalIPs: []
    externalTrafficPolicy: ""

  # job configuration
  job:
    enabled: true
    priorityClassName: ""
    nodeSelector: {}
    affinity: {}
    tolerations: []

  # Configure the ingress resource that allows you to access the vcluster
  ingress:
    # Enable ingress record generation
    enabled: false
    # Ingress path type
    pathType: ImplementationSpecific
    apiVersion: networking.k8s.io/v1
    ingressClassName: ""
    host: vcluster.local
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/backend-protocol: HTTPS
      nginx.ingress.kubernetes.io/ssl-passthrough: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"

  # Set "enable" to true when running vcluster in an OpenShift host
  # This will add an extra rule to the deployed role binding in order 
  # to manage service endpoints
  openshift:
    enable: false

  # If enabled will deploy the coredns configmap
  coredns:
    enabled: false

    replicas: 1
    # image: my-core-dns-image:latest
    # config: |-
    #   .:1053 {
    #      ...
    # CoreDNS service configurations
    service:
      type: ClusterIP
      # Configuration for LoadBalancer service type
      externalIPs: []
      externalTrafficPolicy: ""
      # Extra Annotations
      annotations: {}
    resources: ""

  # If enabled will deploy vcluster in an isolated mode with pod security
  # standards, limit ranges and resource quotas
  isolation:
    enabled: false
    namespace: null

    podSecurityStandard: baseline

    resourceQuota:
      enabled: true
      quota:
        requests.cpu: 10
        requests.memory: 20Gi
        requests.storage: "100Gi"
        requests.ephemeral-storage: 60Gi
        limits.cpu: 20
        limits.memory: 40Gi
        limits.ephemeral-storage: 160Gi
        services.nodeports: 0
        services.loadbalancers: 1
        count/endpoints: 40
        count/pods: 20
        count/services: 20
        count/secrets: 100
        count/configmaps: 100
        count/persistentvolumeclaims: 20
      scopeSelector:
        matchExpressions:
      scopes:

    limitRange:
      enabled: true
      default:
        ephemeral-storage: 8Gi
        memory: 512Mi
        cpu: "1"
      defaultRequest:
        ephemeral-storage: 3Gi
        memory: 128Mi
        cpu: 100m

    networkPolicy:
      enabled: true
      outgoingConnections:
        ipBlock:
          cidr: 0.0.0.0/0
          except:
            - 100.64.0.0/10
            - 127.0.0.0/8
            - 10.0.0.0/8
            - 172.16.0.0/12
            - 192.168.0.0/16

  # manifests to setup when initializing a vcluster
  init:
    manifests: |-
      ---
    helm: []

